import os
import numpy as np
import pandas as pd
import scipy
import sklearn
import keras
from keras.models import Sequential
import cv2
import seaborn as sns
import matplotlib.pyplot as plt
from skimage import io
%matplotlib inline
plt.style.use('bmh')

url='https://raw.githubusercontent.com/lohitkhanna/EDA-in-python/master/train.csv'
df=pd.read_csv(url)
df.head()


#df.info
df2 = df[[column for column in df if df[column].count() / len(df) >= 0.3]]
df2.shape
#4 columns removed as the np of non nan was less than 30%

del df2['Id']
print("List of dropped columns:", end=" ")
for c in df.columns:
    if c not in df2.columns:
        print(c, end=", ")
        print('\n')

df=df2
print(df['SalePrice'].describe())


plt.figure(figsize=(9, 8))
sns.distplot(df['SalePrice'], color='g', bins=100, hist_kws={'alpha': 0.4})

print(set(df.columns.values.tolist()))

list(set(df.dtypes.tolist()))



df_num = df.select_dtypes(include = ['float64', 'int64'])
df_num.head()

df_num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)

df_num_corr = df_num.corr()['SalePrice'][:-1] # -1 because the latest row is SalePrice
golden_features_list = df_num_corr[abs(df_num_corr) > 0.5].sort_values(ascending=False)
print("There is {} strongly correlated values with SalePrice:\n{}".format(len(golden_features_list), golden_features_list))

for i in range(0, len(df_num.columns), 5):
    sns.pairplot(data=df_num,
                x_vars=df_num.columns[i:i+5],
                y_vars=['SalePrice'])

#So now lets remove these 0 values and repeat the process of finding correlated values:


import operator

individual_features_df = []
for i in range(0, len(df_num.columns) - 1): # -1 because the last column is SalePrice
    tmpDf = df_num[[df_num.columns[i], 'SalePrice']]
    tmpDf = tmpDf[tmpDf[df_num.columns[i]] != 0]
    individual_features_df.append(tmpDf)

all_correlations = {feature.columns[0]: feature.corr()['SalePrice'][0] for feature in individual_features_df}
all_correlations = sorted(all_correlations.items(), key=operator.itemgetter(1))
for (key, value) in all_correlations:
    print("{:>15}: {:>15}".format(key, value))

golden_features_list = [key for key, value in all_correlations if abs(value) >= 0.5]
print("There is {} strongly correlated values with SalePrice:\n{}".format(len(golden_features_list), golden_features_list))

# We can use above features/variables for modelling and now we will check multicollinearity using heatmap to reduce no. of variables
# Statistical we should check with VIF >4 / Variance Inflatio Index
# Similarly Shapiro wilks test to chech whether the variable is normally distributed or not ( Ho : Its is normally distributed)
# For Sample adequacy- go for KMO test "  KMO and Bartlett's Test. This table shows two tests that indicate the suitability of your data for structure detection. The Kaiser-Meyer-Olkin Measure of Sampling Adequacy is a statistic that indicates the proportion of variance in your variables that might be caused by underlying factors."
# For Auto correlation test - Durban Watson test with lag can be performed , means residuals are correlated, means your current value is dependent on previous value
# Auto correlation is a characteristic of data which shows the degree of similarity between the values of the same variables over successive time intervals. ... Autocorrelation is diagnosed using a correlogram (ACF plot) and can be tested using the Durbin-Watson test.

corr = df_num.drop('SalePrice', axis=1).corr() # We already examined SalePrice correlations
plt.figure(figsize=(12, 10))

sns.heatmap(corr[(corr >= 0.5) | (corr <= -0.4)], 
            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,
            annot=True, annot_kws={"size": 8}, square=True);

quantitative_features_list = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF',
    '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',
    'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 
    'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'SalePrice']
df_quantitative_values = df[quantitative_features_list]
df_quantitative_values.head()

features_to_analyse = [x for x in quantitative_features_list if x in golden_features_list]
features_to_analyse.append('SalePrice')
features_to_analyse

fig, ax = plt.subplots(round(len(features_to_analyse) / 3), 3, figsize = (18, 12))

for i, ax in enumerate(fig.axes):
    if i < len(features_to_analyse) - 1:
        sns.regplot(x=features_to_analyse[i],y='SalePrice', data=df[features_to_analyse], ax=ax)

# quantitative_features_list[:-1] as the last column is SalePrice and we want to keep it
categorical_features = [a for a in quantitative_features_list[:-1] + df.columns.tolist() if (a not in quantitative_features_list[:-1]) or (a not in df.columns.tolist())]
df_categ = df[categorical_features]
df_categ.head()

df_not_num = df_categ.select_dtypes(include = ['O'])
print('There is {} non numerical features including:\n{}'.format(len(df_not_num.columns), df_not_num.columns.tolist()))

plt.figure(figsize = (10, 6))
ax = sns.boxplot(x='BsmtExposure', y='SalePrice', data=df_categ)
plt.setp(ax.artists, alpha=.5, linewidth=2, edgecolor="k")
plt.xticks(rotation=45)

plt.figure(figsize = (12, 6))
ax = sns.boxplot(x='SaleCondition', y='SalePrice', data=df_categ)
plt.setp(ax.artists, alpha=.5, linewidth=2, edgecolor="k")
plt.xticks(rotation=45)

fig, axes = plt.subplots(round(len(df_not_num.columns) / 3), 3, figsize=(12, 30))

for i, ax in enumerate(fig.axes):
    if i < len(df_not_num.columns):
        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)
        sns.countplot(x=df_not_num.columns[i], alpha=0.7, data=df_not_num, ax=ax)

fig.tight_layout()

# Apllying Gradient Bossting Regressor with parameter tuning
df.head()


df.shape

#['YearRemodAdd', 'YearBuilt', 'TotRmsAbvGrd', 'FullBath', '1stFlrSF', 'GarageArea', 'TotalBsmtSF', 'GarageCars', '2ndFlrSF', 'GrLivArea', 'OverallQual']

df1=df[['YearRemodAdd', 'YearBuilt', 'TotRmsAbvGrd', 'FullBath', '1stFlrSF', 'GarageArea', 'TotalBsmtSF', 'GarageCars', '2ndFlrSF', 'GrLivArea', 'OverallQual', 'SalePrice']]
df1_train_X = df[['YearRemodAdd', 'YearBuilt', 'TotRmsAbvGrd', 'FullBath', '1stFlrSF', 'GarageArea', 'TotalBsmtSF', 'GarageCars', '2ndFlrSF', 'GrLivArea', 'OverallQual']]
df1_train_Y = df[['SalePrice']]

df1.shape
df1_train_X.shape

from sklearn.model_selection import train_test_split

import lightgbm as lgb

X_train, X_test, y_train, y_test = train_test_split(df1_train_X , df1_train_Y, test_size=0.2, random_state=42)

# create dataset for lightgbm
lgb_train = lgb.Dataset(X_train, y_train)
lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)


params = {
    'boosting_type': 'gbdt',
    'objective': 'regression',
    'metric': {'l2', 'l1'},
    'num_leaves': 31,
    'learning_rate': 0.18,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': 0
}
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=20,
                valid_sets=lgb_eval,
                early_stopping_rounds=50)

# Training model using params
print('Saving model...')
# save model to file
gbm.save_model('model.txt')


from sklearn.metrics import mean_squared_error
print('Starting predicting...')
# predict
y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)
# eval
print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)
from sklearn.metrics import mean_squared_error
print('Calculating RMSE of train data...')
# predict
y_pred_train = gbm.predict(X_train, num_iteration=gbm.best_iteration)
# eval
print('The rmse of Train data is:', mean_squared_error(y_train, y_pred_train) ** 0.5)





from sklearn.metrics import mean_squared_error
print('Calculating RMSE of train data...')
# predict
y_pred_train = gbm.predict(X_train, num_iteration=gbm.best_iteration)
# eval
print('The rmse of Train data is:', mean_squared_error(y_train, y_pred_train) ** 0.5)
#Differnce of 25 % in RMSE of train and test data
#Lets check Range of target variable
#df1['SalePrice'].describe
max_1= df1['SalePrice'].max()
min_1= df1['SalePrice'].min()
range_target=max_1-min_1
print(range_target)
RMSE_Percentage_to_Range = (mean_squared_error(y_test, y_pred) ** 0.5)*100/range_target
print("The RMSE percentage to range",RMSE_Percentage_to_Range,"%")

# Using Para Tuning reduced RMSE from 9% t0 5 %


# You can Further Tune Model and Use other validation techniques such as MAPE but we should stick one for complete 1 model and if incase that Evaluation that doesnot consider Business objective then only change evaluation tech/matrix
